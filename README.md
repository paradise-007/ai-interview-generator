# ğŸ¯ AI Interview Question Generator
### E2M Solutions â€“ Technical Assessment Submission

A Streamlit-based AI tool that generates **role-specific interview questions**, **difficulty-calibrated content**, and **structured evaluation rubrics** for any job role and experience level â€” powered by Claude AI.

---

## âœ¨ Features

| Feature | Status |
|---|---|
| Role-specific technical questions | âœ… |
| Behavioral questions by competency | âœ… |
| Structured 4-criterion evaluation rubric | âœ… |
| Difficulty calibration (Junior / Mid / Senior) | âœ… |
| Individual question regeneration | âœ… |
| Custom skill focus (e.g. "focus on APIs") | âœ… |
| Downloadable interview kit (Markdown) | âœ… |
| Scoring template export | âœ… |
| Interviewer best-practice tips | âœ… |
| Input validation & error handling | âœ… |
| Configurable question count | âœ… |

---

## ğŸš€ Quick Start

### 1. Clone the repository
```bash
git clone https://github.com/your-username/ai-interview-generator.git
cd ai-interview-generator
```

### 2. Install dependencies
```bash
pip install -r requirements.txt
```

### 3. Run the app
```bash
streamlit run app.py
```

### 4. Configure your API key
- Open the app in your browser (default: `http://localhost:8501`)
- Enter your **Anthropic API key** in the sidebar (get one free at [console.anthropic.com](https://console.anthropic.com))
- Select a role, level, and click **Generate**

---

## ğŸ“¦ Dependencies

| Package | Version | Purpose |
|---|---|---|
| `streamlit` | â‰¥ 1.40 | Web UI framework |
| `anthropic` | â‰¥ 0.40 | Claude LLM API client |

No database, no external services, no vector stores â€” fully self-contained.

---

## ğŸ—ï¸ Architecture & Data Flow

```
User Input (Sidebar)
  â”‚
  â–¼
Input Validation & Normalization
  â”‚
  â–¼
Prompt Builder  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  (role, level, focus, n_tech, n_beh â†’ structured prompt with explicit JSON schema)   â”‚
  â–¼                                                                                       â”‚
Anthropic API (Claude claude-opus-4-5-20251101)                                          â”‚
  â”‚                                                                                       â”‚
  â–¼                                                                                       â”‚
JSON Extractor (strip markdown fences â†’ parse â†’ validate)                                â”‚
  â”‚                                                                                       â”‚
  â–¼                                                                                       â”‚
Session State (st.session_state.kit)   â—„â”€â”€â”€â”€ Regenerate Single Question â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚
  â”œâ”€â”€ Tab 1: Technical Questions  (with per-question regenerate button)
  â”œâ”€â”€ Tab 2: Behavioral Questions (with per-question regenerate button)
  â”œâ”€â”€ Tab 3: Evaluation Rubric    (+ scoring template)
  â”œâ”€â”€ Tab 4: Interviewer Tips
  â””â”€â”€ Download Button (Markdown export)
```

---

## ğŸ§  Prompt Design Strategy

### System Prompt
The system prompt establishes Claude as an **expert technical recruiter**, enforces **JSON-only output**, and emphasizes role specificity and avoidance of generic questions.

### Generation Prompt
The generation prompt:
1. Provides explicit **JSON schema** with all required fields
2. Embeds **difficulty calibration guidance** per level in the prompt itself
3. Uses a **focus clause** to narrow question themes when specified
4. Specifies exact **question counts** so the model produces predictable output

### Calibration Logic
```
Junior   â†’ fundamentals, core concepts, basic debugging, simple problem solving
Mid-Level â†’ applied knowledge, trade-offs, debugging real scenarios, collaboration
Senior   â†’ system design, architecture, scalability, mentorship, strategic thinking
```

This guidance is injected directly into the prompt â€” no post-processing filtering needed.

---

## ğŸ“ Output Schema

```json
{
  "role": "string",
  "level": "Junior | Mid-Level | Senior",
  "technical_questions": [
    {
      "id": "int",
      "question": "string",
      "rationale": "string",
      "expected_topics": ["string"],
      "difficulty": "Easy | Medium | Hard"
    }
  ],
  "behavioral_questions": [
    {
      "id": "int",
      "question": "string",
      "competency": "Communication | Ownership | Collaboration | ...",
      "rationale": "string"
    }
  ],
  "evaluation_rubric": [
    {
      "criterion": "string",
      "weight": "string",
      "strong": "string",
      "average": "string",
      "weak": "string",
      "scoring_tip": "string"
    }
  ],
  "interview_tips": ["string"]
}
```

---

## ğŸ›¡ï¸ Error Handling

| Scenario | Handling |
|---|---|
| Empty / short role input | Client-side validation, clear error message |
| Invalid API key | `anthropic.AuthenticationError` â†’ user-facing message |
| Rate limit | `anthropic.RateLimitError` â†’ retry guidance |
| Malformed JSON | Regex-based JSON extractor with fallback, then `json.JSONDecodeError` catch |
| Network / timeout | Generic exception catch with retry guidance |
| Regeneration failure | Per-button error display, does not crash session |

---

## âš ï¸ Known Limitations

1. **No streaming** â€” responses appear all at once after ~10â€“15 seconds
2. **Single-session only** â€” generated kits are stored in `st.session_state` and cleared on page refresh
3. **API cost** â€” each generation uses ~1,500â€“2,000 tokens; regeneration uses ~300 tokens
4. **JSON reliability** â€” very rarely, Claude may produce slightly malformed JSON; the extractor handles most cases but extreme failures will show an error
5. **No PDF export** â€” Markdown export is provided; PDF conversion requires additional dependencies

---

## ğŸ”® Future Improvements

- PDF download using `reportlab` or `weasyprint`
- Multiple roles in a single session (comparison mode)
- Candidate scoring form alongside the rubric
- Persistent storage with SQLite for saving past kits
- Custom rubric criteria
- Streaming responses for faster perceived performance
- Shareable kit links (via Streamlit Cloud + database)

---

## ğŸ“ File Structure

```
ai-interview-generator/
â”œâ”€â”€ app.py              â† Main Streamlit application (UI + LLM logic)
â”œâ”€â”€ requirements.txt    â† Python dependencies
â””â”€â”€ README.md           â† This file
```

---

*Built with â¤ï¸ as part of the E2M Solutions Technical Assessment*
